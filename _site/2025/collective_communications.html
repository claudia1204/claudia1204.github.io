<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">

  <title>集合通信操作学习</title>
  <meta name="description" content="集合通信，指的是分布式进程中，多个进程同时进行某一项操作（也叫规约）时的动作。这个规约操作可以是执行加法、减法、乘法…">
  <meta name="author" content="leopardpan">

  <meta name="twitter:card" content="summary">
  <meta name="twitter:title" content="集合通信操作学习">
  <meta name="twitter:description" content="集合通信，指的是分布式进程中，多个进程同时进行某一项操作（也叫规约）时的动作。这个规约操作可以是执行加法、减法、乘法…">

  <meta property="og:type" content="article">
  <meta property="og:title" content="集合通信操作学习">
  <meta property="og:description" content="集合通信，指的是分布式进程中，多个进程同时进行某一项操作（也叫规约）时的动作。这个规约操作可以是执行加法、减法、乘法…">

  <link rel="icon" type="image/png" href="/images/favicon.png" />
  <link href="/images/favicon.png" rel="shortcut icon" type="image/png">

  <link rel="stylesheet" href="/css/main.css">
  <link href="//netdna.bootstrapcdn.com/font-awesome/4.1.0/css/font-awesome.min.css" rel="stylesheet">

  <link rel="canonical" href="http://localhost:8080/2025/collective_communications.html">
  <link rel="alternate" type="application/rss+xml" title="keep moving" href="http://localhost:8080/feed.xml">

  <meta name="google-site-verification" content="1-1ZlHoRvM0T2FqPbW2S-qLgYXN6rsn52kErlMPd_gw" />

<!-- 站点统计 -->
  <script
  async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js">
  </script>

<!-- 百度统计 -->
  
  <script>
      var _hmt = _hmt || [];
      (function() {
        var hm = document.createElement("script");
        hm.src = "//hm.baidu.com/hm.js?625129f6a6c1cc07fc945a8fb5dbc0b4";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(hm, s);
      })();
  </script>
  

<!-- google 统计 -->
  

  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-2487558142971429"
          crossorigin="anonymous"></script>
</head>


  <body>
    <span class="mobile btn-mobile-menu">
      <div class="nav_container">
         <nav class="nav-menu-item" style = "float:right">
            <i class="nav-menu-item">
              <a href="/archive" title="" class="blog-button">  博客主页
              </a>
            </i>
            
                <i class="nav-menu-item">

                  <a href="/archive" title="archive" class="btn-mobile-menu__icon">
                      所有文章
                  </a>
                </i>
            
                <i class="nav-menu-item">

                  <a href="/tags" title="tags" class="btn-mobile-menu__icon">
                      标签
                  </a>
                </i>
            
                <i class="nav-menu-item">

                  <a href="/about" title="about" class="btn-mobile-menu__icon">
                      关于我
                  </a>
                </i>
            
          </nav>
      </div>
    </span>

    <header class="panel-cover panel-cover--collapsed" style="background-image: url('/images/background-cover.jpg')">
  <div class="panel-main">

    <div class="panel-main__inner panel-inverted">
    <div class="panel-main__content">
        <!-- 头像效果-start -->
        <div class="ih-item circle effect right_to_left">            
            <a href="/#blog" title="前往 keep moving 的主页" class="blog-button">
                <div class="img"><img src="/images/avatar.jpg" alt="img"></div>
                <div class="info">
                    <div class="info-back">
                        <h2> 
                            
                                claudia
                            
                        </h2>
                        <p>
                           
                                frontend / python
                            
                        </p>
                    </div>
                </div>
            </a>
        </div>
        <!-- 头像效果-end -->
        <h1 class="panel-cover__title panel-title"><a href="/#blog" title="link to homepage for keep moving" class="blog-button">keep moving</a></h1>
        
        <hr class="panel-cover__divider" />
        <p class="panel-cover__description">small world</p>
        <hr class="panel-cover__divider panel-cover__divider--secondary" />
        
        
        

        <div class="navigation-wrapper">
          <div>
            <nav class="cover-navigation cover-navigation--primary">
              <ul class="navigation">
                <li class="navigation__item"><a href="/#blog" title="" class="blog-button">博客主页</a></li>
                
                  <li class="navigation__item"><a href="/archive" title="archive">所有文章</a></li>
                
                  <li class="navigation__item"><a href="/tags" title="tags">标签</a></li>
                
                  <li class="navigation__item"><a href="/about" title="about">关于我</a></li>
                
              </ul>
            </nav>
          </div>          
        </div>


        </div>
      </div>
    </div>
    
    
    <div class="panel-cover--overlay cover-clear"></div>
    
  </div>
</header>


    <div class="content-wrapper">
        <div class="content-wrapper__inner">
            <article class="post-container post-container--single" itemscope itemtype="http://schema.org/BlogPosting">
  <header class="post-header">
    <h1 class="post-title">集合通信操作学习</h1>
    <div class="post-meta">
      <img src="/images/calendar.png" width="20px"/> 
      <time datetime="2025-01-27 14:29:41 +0800" itemprop="datePublished" class="post-meta__date date">2025-01-27</time>  
         
      <span id="busuanzi_container_page_pv"> | 阅读：<span id="busuanzi_value_page_pv"></span>次</span>
    </p>
    </div>
  </header>

  <section class="post">
    <p>集合通信，指的是分布式进程中，多个进程同时进行某一项操作（也叫规约）时的动作。这个规约操作可以是执行加法、减法、乘法…</p>

<p>比如多机多gpu，执行数据的规约操作。</p>

<p>其中每个进程有一个唯一id，rank，以0为起始。例如2个进程，则分为rank为0和1</p>

<h2 id="1-规约操作分类">1. 规约操作分类</h2>

<p>规约操作根据规约的形式分为几类：</p>

<ol>
  <li>allgather,</li>
  <li>allreduce</li>
  <li>reduce</li>
  <li>reducescatter</li>
  <li>sendrecv</li>
</ol>

<h3 id="11-allgather">1.1 allgather</h3>

<p>收集所有进程的数据，每个进程都存储一份gather后的结果</p>

<p><img src="../images/../images/2025-1-27-collective_communications.assets/allgather.png" alt="../_images/allgather.png" /></p>

<p>图片来源：https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">import</span> <span class="n">torch</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span> <span class="n">as</span> <span class="n">dist</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">multiprocessing</span> <span class="n">as</span> <span class="n">mp</span>

<span class="n">def</span> <span class="n">run_all_gather</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span><span class="o">:</span>
    <span class="cp"># 初始化分布式环境，使用NCCL后端
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

    <span class="cp"># 设置当前进程使用的GPU
</span>    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">f</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="p">{</span><span class="n">rank</span><span class="p">}</span><span class="err">'</span><span class="p">)</span>
    
    <span class="n">tensor_size</span> <span class="o">=</span> <span class="mi">2</span>  <span class="err">#</span> <span class="err">每个进程创建的张量大小</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>  <span class="err">#</span> <span class="err">确保所有张量使用相同的数据类型</span>
    
    <span class="cp"># 创建每个进程对应的那一部分数据，并确保数据类型一致
</span>    <span class="n">input_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Rank {rank} before all_gather: {input_tensor}"</span><span class="p">)</span>

    <span class="cp"># 收集所有ranks的张量
</span>    <span class="n">gather_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">torch</span><span class="p">.</span><span class="n">empty_like</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="n">in</span> <span class="n">range</span><span class="p">(</span><span class="n">world_size</span><span class="p">)]</span>
    <span class="n">dist</span><span class="p">.</span><span class="n">all_gather</span><span class="p">(</span><span class="n">gather_list</span><span class="p">,</span> <span class="n">input_tensor</span><span class="p">)</span>

    <span class="cp"># 输出all_gather后的结果
</span>    <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Rank {rank} after all_gather: {gather_list}"</span><span class="p">)</span>

    <span class="cp"># 清理资源
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="n">def</span> <span class="n">main</span><span class="p">()</span><span class="o">:</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="mi">4</span>  <span class="err">#</span> <span class="err">示例中使用</span><span class="mi">4</span><span class="err">个进程，但可以根据需要调整</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">world_size</span><span class="o">:</span>
        <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"This example requires at least {world_size} GPUs."</span><span class="p">)</span>
    <span class="k">else</span><span class="o">:</span>
        <span class="n">mp</span><span class="p">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">run_all_gather</span><span class="p">,</span>
                 <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,),</span>
                 <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
                 <span class="n">join</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="o">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<p>运行结果：</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Rank 1 before all_gather: tensor([2., 2.], device='cuda:1')
Rank 0 before all_gather: tensor([1., 1.], device='cuda:0')
Rank 3 before all_gather: tensor([4., 4.], device='cuda:3')
Rank 2 before all_gather: tensor([3., 3.], device='cuda:2')
Rank 0 after all_gather: [tensor([1., 1.], device='cuda:0'), tensor([2., 2.], device='cuda:0'), tensor([3., 3.], device='cuda:0'), tensor([4., 4.], device='cuda:0')]
Rank 2 after all_gather: [tensor([1., 1.], device='cuda:2'), tensor([2., 2.], device='cuda:2'), tensor([3., 3.], device='cuda:2'), tensor([4., 4.], device='cuda:2')]
Rank 1 after all_gather: [tensor([1., 1.], device='cuda:1'), tensor([2., 2.], device='cuda:1'), tensor([3., 3.], device='cuda:1'), tensor([4., 4.], device='cuda:1')]
Rank 3 after all_gather: [tensor([1., 1.], device='cuda:3'), tensor([2., 2.], device='cuda:3'), tensor([3., 3.], device='cuda:3'), tensor([4., 4.], device='cuda:3')]
</code></pre></div></div>

<h3 id="12-allreduce操作">1.2 allreduce操作</h3>

<p><strong>将所有进程的数据进行规约（例如sum操作），然后广播回到每个进程</strong></p>

<p>allreduce代码：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">import</span> <span class="n">torch</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span> <span class="n">as</span> <span class="n">dist</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">multiprocessing</span> <span class="n">as</span> <span class="n">mp</span>

<span class="n">def</span> <span class="n">run_all_reduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span><span class="o">:</span>
    <span class="cp"># 初始化分布式环境，使用NCCL后端
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

    <span class="cp"># 设置当前进程使用的GPU
</span>    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">f</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="p">{</span><span class="n">rank</span><span class="p">}</span><span class="err">'</span><span class="p">)</span>
    
    <span class="n">tensor_size</span> <span class="o">=</span> <span class="mi">8</span>  <span class="err">#</span> <span class="err">每个进程创建的张量大小</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>  <span class="err">#</span> <span class="err">确保所有张量使用相同的数据类型</span>
    
    <span class="cp"># 创建每个进程对应的那一部分数据，并确保数据类型一致
</span>    <span class="n">input_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Rank {rank} before all_reduce: {input_tensor}"</span><span class="p">)</span>

    <span class="cp"># 执行all_reduce操作（求和）
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">all_reduce</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">)</span>

    <span class="cp"># 输出all_reduce后的结果
</span>    <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Rank {rank} after all_reduce: {input_tensor}"</span><span class="p">)</span> 

    <span class="cp"># 清理资源
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="n">def</span> <span class="n">main</span><span class="p">()</span><span class="o">:</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="mi">4</span>  <span class="err">#</span> <span class="err">模拟</span><span class="mi">4</span><span class="err">个进程</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">world_size</span><span class="o">:</span>
        <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"This example requires at least {world_size} GPUs."</span><span class="p">)</span>
    <span class="k">else</span><span class="o">:</span>
        <span class="n">mp</span><span class="p">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">run_all_reduce</span><span class="p">,</span>
                 <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,),</span>
                 <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
                 <span class="n">join</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="o">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<p>运行结果：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Rank 3 before all_reduce: tensor<span class="o">([</span>4., 4., 4., 4., 4., 4., 4., 4.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:3'</span><span class="o">)</span>
Rank 1 before all_reduce: tensor<span class="o">([</span>2., 2., 2., 2., 2., 2., 2., 2.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:1'</span><span class="o">)</span>
Rank 0 before all_reduce: tensor<span class="o">([</span>1., 1., 1., 1., 1., 1., 1., 1.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:0'</span><span class="o">)</span>
Rank 2 before all_reduce: tensor<span class="o">([</span>3., 3., 3., 3., 3., 3., 3., 3.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:2'</span><span class="o">)</span>
Rank 0 after all_reduce: tensor<span class="o">([</span>10., 10., 10., 10., 10., 10., 10., 10.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:0'</span><span class="o">)</span>
Rank 1 after all_reduce: tensor<span class="o">([</span>10., 10., 10., 10., 10., 10., 10., 10.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:1'</span><span class="o">)</span>
Rank 3 after all_reduce: tensor<span class="o">([</span>10., 10., 10., 10., 10., 10., 10., 10.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:3'</span><span class="o">)</span>
Rank 2 after all_reduce: tensor<span class="o">([</span>10., 10., 10., 10., 10., 10., 10., 10.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:2'</span><span class="o">)</span>
</code></pre></div></div>

<h3 id="13-reduce操作">1.3 reduce操作</h3>

<p>所有进程执行reduce操作，最终结果发送到根进程</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">import</span> <span class="n">torch</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span> <span class="n">as</span> <span class="n">dist</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">multiprocessing</span> <span class="n">as</span> <span class="n">mp</span>

<span class="n">def</span> <span class="n">run_reduce</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span><span class="o">:</span>
    <span class="cp"># 初始化分布式环境，使用NCCL后端
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

    <span class="cp"># 设置当前进程使用的GPU
</span>    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">f</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="p">{</span><span class="n">rank</span><span class="p">}</span><span class="err">'</span><span class="p">)</span>
    
    <span class="n">tensor_size</span> <span class="o">=</span> <span class="mi">8</span>  <span class="err">#</span> <span class="err">每个进程创建的张量大小</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>  <span class="err">#</span> <span class="err">确保所有张量使用相同的数据类型</span>
    
    <span class="cp"># 创建每个进程对应的那一部分数据，并确保数据类型一致
</span>    <span class="n">input_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Rank {rank} before reduce: {input_tensor}"</span><span class="p">)</span>

    <span class="cp"># 执行reduce操作（求和），并将结果放在根rank上
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">reduce</span><span class="p">(</span><span class="n">input_tensor</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">rank</span> <span class="o">==</span> <span class="mi">0</span><span class="o">:</span>
        <span class="cp"># 根rank上的输出张量现在包含了所有ranks的归约结果
</span>        <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Root Rank {rank} after reduce: {input_tensor}"</span><span class="p">)</span>
    <span class="k">else</span><span class="o">:</span>
        <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Rank {rank} remains unchanged after reduce: {input_tensor}"</span><span class="p">)</span>

    <span class="cp"># 清理资源
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="n">def</span> <span class="n">main</span><span class="p">()</span><span class="o">:</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="mi">4</span>  <span class="err">#</span> <span class="err">模拟</span><span class="mi">4</span><span class="err">个进程</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">world_size</span><span class="o">:</span>
        <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"This example requires at least {world_size} GPUs."</span><span class="p">)</span>
    <span class="k">else</span><span class="o">:</span>
        <span class="n">mp</span><span class="p">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">run_reduce</span><span class="p">,</span>
                 <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,),</span>
                 <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
                 <span class="n">join</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="o">:</span>
    <span class="n">main</span><span class="p">()</span>
</code></pre></div></div>

<p>运行结果：</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">Rank</span> <span class="mi">1</span> <span class="n">before</span> <span class="n">reduce</span><span class="o">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.],</span> <span class="n">device</span><span class="o">=</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="mi">1</span><span class="err">'</span><span class="p">)</span>
<span class="n">Rank</span> <span class="mi">0</span> <span class="n">before</span> <span class="n">reduce</span><span class="o">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">1</span><span class="p">.,</span> <span class="mi">1</span><span class="p">.,</span> <span class="mi">1</span><span class="p">.,</span> <span class="mi">1</span><span class="p">.,</span> <span class="mi">1</span><span class="p">.,</span> <span class="mi">1</span><span class="p">.,</span> <span class="mi">1</span><span class="p">.,</span> <span class="mi">1</span><span class="p">.],</span> <span class="n">device</span><span class="o">=</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="mi">0</span><span class="err">'</span><span class="p">)</span>
<span class="n">Rank</span> <span class="mi">2</span> <span class="n">before</span> <span class="n">reduce</span><span class="o">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.],</span> <span class="n">device</span><span class="o">=</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="mi">2</span><span class="err">'</span><span class="p">)</span>
<span class="n">Rank</span> <span class="mi">3</span> <span class="n">before</span> <span class="n">reduce</span><span class="o">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.],</span> <span class="n">device</span><span class="o">=</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="mi">3</span><span class="err">'</span><span class="p">)</span>
<span class="n">Rank</span> <span class="mi">1</span> <span class="n">remains</span> <span class="n">unchanged</span> <span class="n">after</span> <span class="n">reduce</span><span class="o">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.,</span> <span class="mi">2</span><span class="p">.],</span> <span class="n">device</span><span class="o">=</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="mi">1</span><span class="err">'</span><span class="p">)</span>
<span class="n">Rank</span> <span class="mi">2</span> <span class="n">remains</span> <span class="n">unchanged</span> <span class="n">after</span> <span class="n">reduce</span><span class="o">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.,</span> <span class="mi">3</span><span class="p">.],</span> <span class="n">device</span><span class="o">=</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="mi">2</span><span class="err">'</span><span class="p">)</span>
<span class="n">Root</span> <span class="n">Rank</span> <span class="mi">0</span> <span class="n">after</span> <span class="n">reduce</span><span class="o">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">10</span><span class="p">.,</span> <span class="mi">10</span><span class="p">.,</span> <span class="mi">10</span><span class="p">.,</span> <span class="mi">10</span><span class="p">.,</span> <span class="mi">10</span><span class="p">.,</span> <span class="mi">10</span><span class="p">.,</span> <span class="mi">10</span><span class="p">.,</span> <span class="mi">10</span><span class="p">.],</span> <span class="n">device</span><span class="o">=</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="mi">0</span><span class="err">'</span><span class="p">)</span>
<span class="n">Rank</span> <span class="mi">3</span> <span class="n">remains</span> <span class="n">unchanged</span> <span class="n">after</span> <span class="n">reduce</span><span class="o">:</span> <span class="n">tensor</span><span class="p">([</span><span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.,</span> <span class="mi">4</span><span class="p">.],</span> <span class="n">device</span><span class="o">=</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="mi">3</span><span class="err">'</span><span class="p">)</span>
</code></pre></div></div>

<h3 id="14-reducescatter-操作">1.4 reducescatter 操作</h3>

<p>每个进程拥有相同大小的张量数据，reducescatter会先对所有数据执行reduce 操作，然后每个进程接收一部分结果</p>

<p><img src="../images/../images/2025-1-27-collective_communications.assets/reducescatter.png" alt="../_images/reducescatter.png" /></p>

<p>图片来源：https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/collectives.html</p>

<div class="language-c highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">import</span> <span class="n">torch</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">distributed</span> <span class="n">as</span> <span class="n">dist</span>
<span class="n">import</span> <span class="n">torch</span><span class="p">.</span><span class="n">multiprocessing</span> <span class="n">as</span> <span class="n">mp</span>

<span class="n">def</span> <span class="n">run_reduce_scatter</span><span class="p">(</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span><span class="o">:</span>
    <span class="cp"># 初始化分布式环境，使用NCCL后端
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">init_process_group</span><span class="p">(</span><span class="s">"nccl"</span><span class="p">,</span> <span class="n">rank</span><span class="o">=</span><span class="n">rank</span><span class="p">,</span> <span class="n">world_size</span><span class="o">=</span><span class="n">world_size</span><span class="p">)</span>

    <span class="cp"># 设置当前进程使用的GPU
</span>    <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">device</span><span class="p">(</span><span class="n">f</span><span class="err">'</span><span class="n">cuda</span><span class="o">:</span><span class="p">{</span><span class="n">rank</span><span class="p">}</span><span class="err">'</span><span class="p">)</span>
    
    <span class="n">tensor_size</span> <span class="o">=</span> <span class="mi">8</span>  <span class="err">#</span> <span class="err">每个进程创建的张量大小</span>
    <span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">float32</span>  <span class="err">#</span> <span class="err">确保所有张量使用相同的数据类型</span>
    
    <span class="cp"># 创建每个进程对应的那一部分数据，并确保数据类型一致
</span>    <span class="n">input_tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">ones</span><span class="p">(</span><span class="n">tensor_size</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">rank</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)).</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>
    <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Rank {rank} before reduce_scatter: {input_tensor}"</span><span class="p">)</span>

    <span class="cp"># 准备接收结果的张量，并确保其数据类型与输入张量一致
</span>    <span class="n">output_tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">empty</span><span class="p">(</span><span class="n">tensor_size</span> <span class="c1">// world_size, dtype=dtype).to(device)</span>

    <span class="cp"># 执行reduce_scatter操作（求和）
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">reduce_scatter</span><span class="p">(</span><span class="n">output_tensor</span><span class="p">,</span> <span class="p">[</span><span class="n">input_tensor</span><span class="p">],</span> <span class="n">op</span><span class="o">=</span><span class="n">dist</span><span class="p">.</span><span class="n">ReduceOp</span><span class="p">.</span><span class="n">SUM</span><span class="p">)</span>

    <span class="cp"># 输出reduce_scatter后的结果
</span>    <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"Rank {rank} after reduce_scatter: {output_tensor}"</span><span class="p">)</span>

    <span class="cp"># 清理资源
</span>    <span class="n">dist</span><span class="p">.</span><span class="n">destroy_process_group</span><span class="p">()</span>

<span class="n">def</span> <span class="n">main</span><span class="p">()</span><span class="o">:</span>
    <span class="n">world_size</span> <span class="o">=</span> <span class="mi">4</span>  <span class="err">#</span> <span class="err">模拟</span><span class="mi">4</span><span class="err">个进程</span>
    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="n">device_count</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">world_size</span><span class="o">:</span>
        <span class="n">print</span><span class="p">(</span><span class="n">f</span><span class="s">"This example requires at least {world_size} GPUs."</span><span class="p">)</span>
    <span class="k">else</span><span class="o">:</span>
        <span class="n">mp</span><span class="p">.</span><span class="n">spawn</span><span class="p">(</span><span class="n">run_reduce_scatter</span><span class="p">,</span>
                 <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">world_size</span><span class="p">,),</span>
                 <span class="n">nprocs</span><span class="o">=</span><span class="n">world_size</span><span class="p">,</span>
                 <span class="n">join</span><span class="o">=</span><span class="n">True</span><span class="p">)</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">"__main__"</span><span class="o">:</span>
    <span class="n">main</span><span class="p">()</span>
    
</code></pre></div></div>

<p>运行结果：</p>

<div class="language-shell highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Rank 1 before reduce_scatter: tensor<span class="o">([</span>2., 2., 2., 2., 2., 2., 2., 2.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:1'</span><span class="o">)</span>
Rank 2 before reduce_scatter: tensor<span class="o">([</span>3., 3., 3., 3., 3., 3., 3., 3.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:2'</span><span class="o">)</span>
Rank 3 before reduce_scatter: tensor<span class="o">([</span>4., 4., 4., 4., 4., 4., 4., 4.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:3'</span><span class="o">)</span>
Rank 0 before reduce_scatter: tensor<span class="o">([</span>1., 1., 1., 1., 1., 1., 1., 1.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:0'</span><span class="o">)</span>
Rank 1 after reduce_scatter: tensor<span class="o">([</span>10., 10.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:1'</span><span class="o">)</span>
Rank 3 after reduce_scatter: tensor<span class="o">([</span>10., 10.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:3'</span><span class="o">)</span>
Rank 2 after reduce_scatter: tensor<span class="o">([</span>10., 10.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:2'</span><span class="o">)</span>
Rank 0 after reduce_scatter: tensor<span class="o">([</span>10., 10.], <span class="nv">device</span><span class="o">=</span><span class="s1">'cuda:0'</span><span class="o">)</span>
</code></pre></div></div>



  </section>
</article>

<section>

            <div class="content-play">
              <p><a href="javascript:void(0)" onclick="dashangToggle()" class="dashang" title="打赏，支持一下">打赏一个呗</a></p>
              <div class="hide_box-play"></div>
              <div class="shang_box-play">
                <a class="shang_close-play" href="javascript:void(0)" onclick="dashangToggle()" title="关闭"><img src="/images/payimg/close.jpg" alt="取消" /></a>
                <div class="shang_tit-play">
                  <p>感谢您的支持，我会继续努力的!</p>
                </div>
                <div class="shang_payimg">
                    <img src="/images/payimg/alipayimg.jpg" alt="扫码支持" title="扫一扫" />
                </div>
              <div class="shang_payimg">    
                    <img src="/images/payimg/weipayimg.jpg" alt="扫码支持" title="扫一扫" />
                </div>
                <div class="pay_explain">扫码打赏，你说多少就多少</div>
                <div class="shang_payselect">
                  <div class="pay_item checked" data-id="alipay">
                    <span class="pay_logo"><img src="/images/payimg/alipay.jpg" alt="支付宝" /></span>
                  </div>
                  <div class="pay_item" data-id="weipay">
                    <span class="pay_logo"><img src="/images/payimg/wechat.jpg" alt="微信" /></span>
                  </div>
                </div>
                <div class="shang_info-play">
                  <p>打开<span id="shang_pay_txt">支付宝</span>扫一扫，即可进行扫码打赏哦</p>
                </div>
              </div>
            </div>
            <script type="text/javascript">
            function dashangToggle(){
              $(".hide_box-play").fadeToggle();
              $(".shang_box-play").fadeToggle();
            }
            </script>

            <div style="text-align:center;margin:50px 0; font:normal 14px/24px 'MicroSoft YaHei';"></div>

            <style type="text/css">
              .content-play{width:80%;margin-top: 20px;margin-bottom: 10px;height:40px;}
              .hide_box-play{z-index:999;filter:alpha(opacity=50);background:#666;opacity: 0.5;-moz-opacity: 0.5;left:0;top:0;height:99%;width:100%;position:fixed;display:none;}
              .shang_box-play{width:540px;height:540px;padding:10px;background-color:#fff;border-radius:10px;position:fixed;z-index:1000;left:50%;top:50%;margin-left:-280px;margin-top:-280px;border:1px dotted #dedede;display:none;}
              .shang_box-play img{border:none;border-width:0;}
              .dashang{display:block;width:100px;margin:5px auto;height:25px;line-height:25px;padding:10px;background-color:#E74851;color:#fff;text-align:center;text-decoration:none;border-radius:10px;font-weight:bold;font-size:16px;transition: all 0.3s;}
              .dashang:hover{opacity:0.8;padding:15px;font-size:18px;}
              .shang_close-play{float:right;display:inline-block;
                margin-right: 10px;margin-top: 20px;
              }
              .shang_logo{display:block;text-align:center;margin:20px auto;}
              .shang_tit-play{width: 100%;height: 75px;text-align: center;line-height: 66px;color: #a3a3a3;font-size: 16px;background: url('/images/payimg/cy-reward-title-bg.jpg');font-family: 'Microsoft YaHei';margin-top: 7px;margin-right:2px;}
              .shang_tit-play p{color:#a3a3a3;text-align:center;font-size:16px;}
              .shang_payimg{width:140px;padding:10px;padding-left: 80px; /*border:6px solid #EA5F00;**/margin:0 auto;border-radius:3px;height:140px;display:inline-block;}
              .shang_payimg img{display:inline-block;margin-right:10px;float:left;text-align:center;width:140px;height:140px; }
              .pay_explain{text-align:center;margin:10px auto;font-size:12px;color:#545454;}
              .shang_payselect{text-align:center;margin:0 auto;margin-top:40px;cursor:pointer;height:60px;width:500px;margin-left:110px;}
              .shang_payselect .pay_item{display:inline-block;margin-right:140px;float:left;}
              .shang_info-play{clear:both;}
              .shang_info-play p,.shang_info-play a{color:#C3C3C3;text-align:center;font-size:12px;text-decoration:none;line-height:2em;}
            </style>

       <ul class="pager">
        
        <li class="previous">
            <a href="/2024/c_2d_array.html" data-toggle="tooltip" data-placement="top" title="c语言：二维数组传参">上一篇：  <span>c语言：二维数组传参</span>
            </a>
        </li>
        
        
        <li class="next">
            <a href="/2025/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F%E7%9B%B8%E5%85%B3%E5%AD%A6%E4%B9%A0.html" data-toggle="tooltip" data-placement="top" title="操作系统相关学习">下一篇：  <span>操作系统相关学习</span>
            </a>
        </li>
        
    </ul>
</section>

<section class="post-comments">

  

</section>


            <section class="footer">
    <footer>
        <div class = "footer_div">  
        <nav class="cover-navigation navigation--social">
          <ul class="navigation">

          

          
          
          

          

          <!-- RSS -->
          <li class="navigation__item_social">
            <a href="/feed.xml" rel="author" title="RSS" target="_blank">
              <i class='social fa fa-rss fa-2x'></i>
              <span class="label">RSS</span>
            </a>
          </li>

          

          </ul>
        </nav>

        </div>

        <div class = "footer_div">  
           <p class="copyright text-muted">
            Copyright &copy; keep moving 2025 Theme by <a href="http://claudia1204.github.io/">claudia1204</a> |
            <iframe
                style="margin-left: 2px; margin-bottom:-5px;"
                frameborder="0" scrolling="0" width="91px" height="20px"
                src="https://ghbtns.com/github-btn.html?user=claudia1204&repo=claudia1204.github.io&type=star&count=true" >
            </iframe>
            </p>
        	<div align="right">
    			<link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.3.0/css/font-awesome.min.css">

          <!-- 访问统计 -->
          <span id="busuanzi_container_site_pv">
            本站总访问量
            <span id="busuanzi_value_site_pv"></span>次
          </span>

        </div>
        <div>
    </footer>
</section>

        </div>
    </div>

    <script type="text/javascript" src="//code.jquery.com/jquery-1.11.3.min.js"></script>
<script type="text/javascript" src="/js/main.js"></script>

<script type="text/javascript" src="/js/highlight.pack.js"></script>
<script>hljs.initHighlightingOnLoad();</script>



  </body>

</html>
